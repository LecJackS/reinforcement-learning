{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the same as *Policy Evaluation Solution.ipynb*, just that contains detailed explanation of each line or couple of lines and a couple of extra prints to visualize better the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these DP problems, we'll assume complete knowledge of the environment dynamics. I.e, we can predict what will happen if we do action 'a' on every state 's' from the beginning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from lib.envs.gridworld import GridworldEnv\n",
    "# Importing gridworld environment. It's a modification of Open AI Gym's Toy_Text \"Discrete\", following\n",
    "# Suttons example from chapter 4 (pdf pg. 100) of his Reinforcement Learning book.\n",
    "# Original: https://github.com/openai/gym/blob/master/gym/envs/toy_text/discrete.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sutton's *Reinforcement Learning: An Introduction*: https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf\n",
    "\n",
    "Original env: https://github.com/openai/gym/blob/master/gym/envs/toy_text/discrete.py\n",
    "\n",
    "Gridworld env: https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/gridworld.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pp = pprint.PrettyPrinter(indent=2)\n",
    "# I think it's not used here\n",
    "env = GridworldEnv()\n",
    "# now env is our gridworld environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gridworld-from-Book](../images/DP-PE-Gridworld.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Policy Evaluation\n",
    "### Pseudocode (copy-paste from the book)\n",
    "\n",
    "Input $\\pi$, the policy to be evaluated\n",
    "\n",
    "Initialize an array $V(s)=0$, for all $s\\in S^+$\n",
    "\n",
    "Repeat\n",
    "\n",
    ". . . . $\\Delta \\leftarrow 0$\n",
    "    \n",
    ". . . . For each $s\\in S$:\n",
    "        \n",
    ". . . . . . . . $v \\leftarrow V(s)$\n",
    "        \n",
    ". . . . . . . . $V(s) \\leftarrow \\sum_{a} \\pi(a|s) \\sum_{s',r} p(s',r|s,a)[r + \\gamma V(s')]$\n",
    "        \n",
    ". . . . . . . . $\\Delta \\leftarrow max( \\Delta, \\left|v - V (s)\\right| )$\n",
    "        \n",
    "until $\\Delta < \\theta$ (a small positive number)\n",
    "    \n",
    "Output $V \\approx v_\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gridworld-from-Book](../images/DP-Policy-Evaluation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Origin of V(s) function:\n",
    "\n",
    "$V_{ \\pi } (s) = \\sum_{a} \\pi (a|s) * q_\\pi (s,a)$\n",
    "\n",
    "$q_{\\pi } (s,a) = R_a^s + \\gamma \\sum_{s'\\in S} P_{ss'}^a v_\\pi (s)$\n",
    "\n",
    "$V(s) = \\sum_{a} \\pi (a|s) \\sum_{s',r} p(s',r|s,a)[r + \\gamma V(s')]$\n",
    "\n",
    "For a more detailed and clear explanation, watch *RL Course by David Silver - Lecture 2: Markov Decision Process* @ 53m36s\n",
    "https://youtu.be/lfHX2hHRMVQ?t=53m36s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a (prob, next_state, reward, done) tuple.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: lambda discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    # Initialize an array V(s)=0, for all s∈S+\n",
    "    # It's going to save a value for every state as a vector here\n",
    "    V = np.zeros(env.nS)\n",
    "    i = 0\n",
    "    # Setting decimal points to better visualizing\n",
    "    np.set_printoptions(precision=2)\n",
    "    print(\"Grid Value Function at iteration: \", i)\n",
    "    print(np.array(V).reshape(env.shape))\n",
    "    print(\"\")\n",
    "    while True:\n",
    "        # ∆ ← 0\n",
    "        delta = 0\n",
    "        i += 1\n",
    "        # For each state, perform a \"full backup\"\n",
    "        # For each s∈S:\n",
    "        for s in range(env.nS):\n",
    "            # from position 0 to 15 of the grid\n",
    "            v = 0\n",
    "            # Look at the possible next actions\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                # 'a' can be 0, 1, 2 or 3\n",
    "                # with random_policy, action_prob is always 0.25 for each 'a'\n",
    "                # For each action, look at the possible next states...\n",
    "                # [s] will be position in grid\n",
    "                # [a] will be action taked\n",
    "                for  prob, next_state, reward, done in env.P[s][a]:\n",
    "                    # for example, if in position 0 of grid, every action will lead to terminal state\n",
    "                    # eg. if in position 1 of grid:\n",
    "                    #     a=0(up) will end up on same position with 100% chance\n",
    "                    #     a=1(right) will end up on grid position 2 with 100% chance\n",
    "                    #     a=2(down) will end up on grid position 5 (below pos.1) with 100% chance\n",
    "                    #     a=3(left) will end up on grid position 0 with 100% chance, which is terminal\n",
    "                    # each transition but terminal state has a reward of -1\n",
    "                    #print(\"s:\",s, \"a:\",a, \"prob:\", prob, \"next:\", next_state, \"reward:\", reward, \"done:\",done)\n",
    "                    # Calculate the expected value\n",
    "                    # V(s) ← Sum_{a} π(a|s) Sum_{s',r} p(s',r|s,a)[r + γV(s')]\n",
    "                    v += action_prob * prob * (reward + discount_factor * V[next_state])\n",
    "                    # it saves the expected value in v instead of V(s), so it can sum over actions and\n",
    "                    # next_states and rewards, but later copy v into V[s] and repeat for every state/grid position\n",
    "            # How much our value function changed (across any states)\n",
    "            # ∆ ← max( ∆, |v − V (s)| )\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            # Now copies v into V(s)\n",
    "            V[s] = v\n",
    "        # Stop evaluating once our value function change is below a threshold\n",
    "        # until ∆ < θ (a small positive number)\n",
    "        if delta < theta:\n",
    "            break\n",
    "        if  i==1 or i%20 == 0:\n",
    "            print(\"Grid Value Function at iteration: \", i)\n",
    "            print(np.array(V).reshape(env.shape))\n",
    "            print(\"delta: \", delta)\n",
    "            print(\"\")\n",
    "    #Output V ≈ v_π\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Value Function at iteration:  0\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "\n",
      "Grid Value Function at iteration:  1\n",
      "[[ 0.   -1.   -1.25 -1.31]\n",
      " [-1.   -1.5  -1.69 -1.75]\n",
      " [-1.25 -1.69 -1.84 -1.9 ]\n",
      " [-1.31 -1.75 -1.9   0.  ]]\n",
      "delta:  1.8984375\n",
      "\n",
      "Grid Value Function at iteration:  20\n",
      "[[  0.   -11.43 -16.3  -17.93]\n",
      " [-11.43 -14.84 -16.57 -16.61]\n",
      " [-16.3  -16.57 -15.11 -11.84]\n",
      " [-17.93 -16.61 -11.84   0.  ]]\n",
      "delta:  0.372594031843\n",
      "\n",
      "Grid Value Function at iteration:  40\n",
      "[[  0.   -13.55 -19.36 -21.29]\n",
      " [-13.55 -17.45 -19.4  -19.41]\n",
      " [-19.36 -19.4  -17.5  -13.62]\n",
      " [-21.29 -19.41 -13.62   0.  ]]\n",
      "delta:  0.0647083950673\n",
      "\n",
      "Grid Value Function at iteration:  60\n",
      "[[  0.   -13.92 -19.89 -21.88]\n",
      " [-13.92 -17.9  -19.9  -19.9 ]\n",
      " [-19.89 -19.9  -17.91 -13.93]\n",
      " [-21.88 -19.9  -13.93   0.  ]]\n",
      "delta:  0.0112378558487\n",
      "\n",
      "Grid Value Function at iteration:  80\n",
      "[[  0.   -13.99 -19.98 -21.98]\n",
      " [-13.99 -17.98 -19.98 -19.98]\n",
      " [-19.98 -19.98 -17.98 -13.99]\n",
      " [-21.98 -19.98 -13.99   0.  ]]\n",
      "delta:  0.00195166954678\n",
      "\n",
      "Grid Value Function at iteration:  100\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "delta:  0.000338944908272\n",
      "\n",
      "Grid Value Function at iteration:  120\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "delta:  5.88642944344e-05\n",
      "\n",
      "Grid Value Function at iteration:  140\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "delta:  1.02229155097e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Input π, the policy to be evaluated\n",
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "# value of policy\n",
    "v = policy_eval(random_policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function:\n",
      "[  0. -14. -20. -22. -14. -18. -20. -20. -20. -20. -18. -14. -22. -20. -14.\n",
      "   0.]\n",
      "\n",
      "Reshaped Grid Value Function:\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "#Grid values. Compare to image.\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gridworld-OptimalPolicy-from-Book](../images/DP-PE-OptimalPolicy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test: Make sure the evaluated policy is what we expected\n",
    "expected_v = np.array([0, -14, -20, -22, -14, -18, -20, -20, -20, -20, -18, -14, -22, -20, -14, 0])\n",
    "np.testing.assert_array_almost_equal(v, expected_v, decimal=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
